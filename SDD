1.	Solution:
	a.	Environment and data set creation
		i.	Set up an AWS account and create an S3 bucket named “input data”.
		ii.	Upload the given sample data to the “input-data” folder in the S3 bucket.
		iii.	Set up AWS Redshift, Databricks, and EMR studio.
		iv.	Create a GitHub repository to manage code
	b.	Cleaning Activity 
		i.	Create PySpark scripts in data bricks to clean the data. Use PySpark’s DataFrame API 
		ii.	Use the following command to connect your databricks to s3 and add your access key and secret key
			1.	spark.sparkContext._jsc.hadoopConfiguration().set("fs.s3a.access.key", "")
			2.	spark.sparkContext._jsc.hadoopConfiguration().set("fs.s3a.secret.key", "")
		iii.	Load the datasets from the S3 bucket into PySpark DataFrames
		iv.	Check for null values in each dataset using the .isNull() function
		v.	Count the total null values for each column using the .agg() function.
		vi.	Replace null values with "NA" or appropriate values using the .fillna() function.
		vii.	Drop duplicate records using the .dropDuplicates() function.
		viii.	Clean the required datasets: Patients, Subscriber, Claims, Group_subgroup.
		ix.	Create cleaned DataFrames for each cleaned dataset.
	c.	Redshift table Schema design
		i.	Create a schema  that shows the structure of the redshift structure table for each cleaned DataFrames. It should include table names, 
				column names, data types, primary key, and foreign key. 
		ii.	Use the Create Table command to create new tables. 
	d.	Data Loading to RedShift
		i.	Load cleaned data from DataBricks to redshift tables 
		ii.	Use the following command and update the command with the proper credentials 
			1.	spark.sparkContext._jsc.hadoopConfiguration().set("fs.s3a.access.key", "")
			2.	spark.sparkContext._jsc.hadoopConfiguration().set("fs.s3a.secret.key", "")
			3.	df. write.format("redshift").option("url", "jdbc:redshift://default-workgroup.556646946508.us-east-1.redshift-serverless.amazonaws.com:5439/dev").
					option("dbtable", "test.capstone -project").option("aws_iam_role", "").option("driver","com.amazon.redshift.jdbc42.Driver").option("tempdir", "s3a:// / ").
					option("user", "admin").option("password", " mode("overwrite").save()
	e.	Analysis and Result Creation
		i.	Create a schema “Project-output”.
		ii.	Write an SQL on those redshift tables (cleaned data table)
		iii.	One destination table for each use case 
	f.	Testing and Deploying
		i.	Test your SQL queries in Databricks to visualize the output of SQL
		ii.	Push the tested code to the GitHub repository. 
		iii.	Deploy it to AWS EMR using that code

2.	Use Cases 
	a.	Identifying High-Claim Diseases
	b.	Targeting Young Subscribers
	c.	Optimizing Group Subgroup Offerings
	d.	Optimizing Hospital Services
	e.	Subgroup Subscription Patterns
	f.	Claim Rejection Analysis
	g.	Geographical Claims Analysis
	h.	Government vs. Private Policy Preference
	i.	Average Premium Pay Analysis
	j.	Profitable Group Identification
	k.	Young cancer patients
	l. 	High-Expense Cashless Insurance
	m. 	Old-aged female Knee Surgery Patients



3.	Database Design -

	a.	Tables Metadata Info with Pk/FK relationship -
		i.	Patients
			Columns: patient_id (PK), patient_name, patient_gender, patient_phone, disease_name, city, hospital_id (FK)

		ii.	Subscribers
			Columns: sub_id (PK), first_name, last_name, Street, Birth_date, Gender, Phone, Country, City, Zip_Code, Subgrp_id (FK), Elig_ind, eff_date, term_date

		iii.	Claims
			Columns: claim_id (PK), patient_id (FK), disease_name, SUB_ID (FK), CLaim_Or_Rejected, claim_Type, claim_amount, claim_date

		iv. 	Subgroup
			Columns: SubGrp_id (PK), SubGrp_Name, Monthly_Premium

		v. 	Disease
			Columns: SubGrpID (FK), Disease_ID (PK), Disease_name

		vi.	Hospital
			Columns: Hospital_id (PK), Hospital_name, city, state, country

		vii.	group
			Columns: Country, premium_written, zipcode, Grp_Id (PK), Grp_Name, Grp_Type, city, year

			
4.	Technologies and Platforms to be used in this solution 

	AWS S3
	AWS Redshift
	Databricks
	AWS EMR Studio
	Pyspark
	Jira
	GitHub


